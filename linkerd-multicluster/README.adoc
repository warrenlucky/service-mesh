=== Create cluster in gcp

** Manually create gke cluster and rename cluster name as east, west
*** enable access for two cluster like
----
gcloud container clusters get-credentials devops3 --zone us-central1-c --project fr-dev-piiworker
----

*** rename
----
kubectl config rename-context gke_fr-dev-piiworker_us-central1-c_devops-2 east
kubectl config rename-context gke_fr-dev-piiworker_us-central1-c_devops3 west
----

**** We like to use the step CLI to generate these certificates. If you prefer openssl instead, feel free to use that! To generate the trust anchor with step
**** install step first
----
wget -O step-cli.tar.gz https://github.com/smallstep/cli/releases/download/v0.15.3/step_linux_0.15.3_amd64.tar.gz
tar -xf step-cli.tar.gz
sudo cp step_0.15.3/bin/step /usr/bin
----

**** generate certificates

----
[root@devopstesting linkerd]# step certificate create root.linkerd.cluster.local root.crt root.key \
>   --profile root-ca --no-password --insecure
Your certificate has been saved in root.crt.
Your private key has been saved in root.key.
----

**** To generate the issuer credentials using the trust anchor, run:
----
[root@devopstesting linkerd]# step certificate create identity.linkerd.cluster.local issuer.crt issuer.key \
>   --profile intermediate-ca --not-after 8760h --no-password --insecure \
>   --ca root.crt --ca-key root.key
Your certificate has been saved in issuer.crt.
Your private key has been saved in issuer.key.
----

**** An identity service in your cluster will use the certificate and key that you generated here to generate the certificates that each individual proxy uses. While we will be using the same issuer credentials on each cluster for this guide, it is a good idea to have separate ones for each cluster. Read through the certificate documentation for more details.

With a valid trust anchor and issuer credentials, we can install Linkerd on your west and east clusters now. will create lots things by below command

----
linkerd install \
  --identity-trust-anchors-file root.crt \
  --identity-issuer-certificate-file issuer.crt \
  --identity-issuer-key-file issuer.key \
  | tee \
    >(kubectl --context=west apply -f -) \
    >(kubectl --context=east apply -f -)
----

**** check things here by 

----
for ctx in west east; do
  echo "Checking cluster: ${ctx} .........\n"
  linkerd --context=${ctx} check || break
  echo "-------------\n"
done
----

**** To install the multicluster components on both west and east, you can run:

----
for ctx in west east; do
  echo "Installing on cluster: ${ctx} ........."
  linkerd --context=${ctx} multicluster install | \
    kubectl --context=${ctx} apply -f - || break
  echo "-------------\n"
done

Installing on cluster: west .........
namespace/linkerd-multicluster created
configmap/linkerd-gateway-config created
deployment.apps/linkerd-gateway created
service/linkerd-gateway created
serviceaccount/linkerd-gateway created
clusterrole.rbac.authorization.k8s.io/linkerd-service-mirror-remote-access-default created
serviceaccount/linkerd-service-mirror-remote-access-default created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-service-mirror-remote-access-default created
customresourcedefinition.apiextensions.k8s.io/links.multicluster.linkerd.io created
-------------\n
Installing on cluster: east .........
namespace/linkerd-multicluster created
configmap/linkerd-gateway-config created
deployment.apps/linkerd-gateway created
service/linkerd-gateway created
serviceaccount/linkerd-gateway created
clusterrole.rbac.authorization.k8s.io/linkerd-service-mirror-remote-access-default created
serviceaccount/linkerd-service-mirror-remote-access-default created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-service-mirror-remote-access-default created
customresourcedefinition.apiextensions.k8s.io/links.multicluster.linkerd.io created
-------------\n
----

**** Make sure gateway successfully deployed.

----
[root@devopstesting linkerd]# for ctx in west east; do
>   echo "Checking gateway on cluster: ${ctx} ........."
>   kubectl --context=${ctx} -n linkerd-multicluster \
>     rollout status deploy/linkerd-gateway || break
>   echo "-------------\n"
> done
Checking gateway on cluster: west .........
deployment "linkerd-gateway" successfully rolled out
-------------\n
Checking gateway on cluster: east .........
deployment "linkerd-gateway" successfully rolled out
-------------\n
----


**** Double check that the load balancer was able to allocate a public IP address by running:

----
[root@devopstesting linkerd]# for ctx in west east; do
>   printf "Checking cluster: ${ctx} ........."
>   while [ "$(kubectl --context=${ctx} -n linkerd-multicluster get service \
>     -o 'custom-columns=:.status.loadBalancer.ingress[0].ip' \
>     --no-headers)" = "<none>" ]; do
>       printf '.'
>       sleep 1
>   done
>   printf "\n"
> done
Checking cluster: west .........
Checking cluster: east .........

[root@devopstesting linkerd]# kubectl get svc -n linkerd-multicluster
NAME              TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                         AGE
linkerd-gateway   LoadBalancer   10.108.13.37   35.222.122.155   4143:32392/TCP,4181:30886/TCP   7m40s
----

**** To link the west cluster to the east one, run:

----
linkerd --context=east multicluster link --cluster-name east |
  kubectl --context=west apply -f -
  
secret/cluster-credentials-east created
link.multicluster.linkerd.io/east created
clusterrole.rbac.authorization.k8s.io/linkerd-service-mirror-access-local-resources-east created
clusterrolebinding.rbac.authorization.k8s.io/linkerd-service-mirror-access-local-resources-east created
role.rbac.authorization.k8s.io/linkerd-service-mirror-read-remote-creds-east created
rolebinding.rbac.authorization.k8s.io/linkerd-service-mirror-read-remote-creds-east created
serviceaccount/linkerd-service-mirror-east created
deployment.apps/linkerd-service-mirror-east created
service/probe-gateway-east created
----

**** Linkerd will look at your current east context, extract the cluster configuration which contains the server location as well as the CA bundle. It will then fetch the ServiceAccount token and merge these pieces of configuration into a kubeconfig that is a secret.

Running check again will make sure that the service mirror has discovered this secret and can reach east.
----
linkerd --context=west check --multicluster
output ...
linkerd-multicluster
--------------------
√ Link CRD exists
√ Link resources are valid
        * east
√ remote cluster access credentials are valid
        * east
√ clusters share trust anchors
        * east
√ service mirror controller has required permissions
        * east
√ service mirror controllers are running
        * east
√ all gateway mirrors are healthy
        * east
Status check results are √
----
**** Additionally, the east gateway should now show up in the list:

----
[root@devopstesting linkerd]# linkerd --context=west multicluster gateways
CLUSTER  ALIVE    NUM_SVC  LATENCY_P50  LATENCY_P95  LATENCY_P99  
east     True           0          4ms          5ms          5ms  
----

=== Test multi-cluster

**** It is time to test this all out! The first step is to add some services that we can mirror. To add these to both clusters, you can run:

----
for ctx in west east; do
  echo "Adding test services on cluster: ${ctx} ........."
  kubectl --context=${ctx} apply \
    -k "github.com/linkerd/website/multicluster/${ctx}/"
  kubectl --context=${ctx} -n test \
    rollout status deploy/podinfo || break
  echo "-------------\n"
done

Adding test services on cluster: west .........
namespace/test created
configmap/frontend created
service/frontend created
service/podinfo created
deployment.apps/frontend created
deployment.apps/podinfo created
horizontalpodautoscaler.autoscaling/podinfo created
Waiting for deployment "podinfo" rollout to finish: 0 of 1 updated replicas are available...
Waiting for deployment spec update to be observed...
Waiting for deployment "podinfo" rollout to finish: 1 out of 2 new replicas have been updated...
Waiting for deployment "podinfo" rollout to finish: 1 out of 2 new replicas have been updated...
Waiting for deployment "podinfo" rollout to finish: 0 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 0 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 1 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 1 of 2 updated replicas are available...
deployment "podinfo" successfully rolled out
-------------\n
Adding test services on cluster: east .........
namespace/test created
configmap/frontend created
service/frontend created
service/podinfo created
deployment.apps/frontend created
deployment.apps/podinfo created
horizontalpodautoscaler.autoscaling/podinfo created
Waiting for deployment spec update to be observed...
Waiting for deployment spec update to be observed...
Waiting for deployment "podinfo" rollout to finish: 0 out of 1 new replicas have been updated...
Waiting for deployment "podinfo" rollout to finish: 0 of 1 updated replicas are available...
Waiting for deployment spec update to be observed...
Waiting for deployment "podinfo" rollout to finish: 1 out of 2 new replicas have been updated...
Waiting for deployment "podinfo" rollout to finish: 1 out of 2 new replicas have been updated...
Waiting for deployment "podinfo" rollout to finish: 0 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 0 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 1 of 2 updated replicas are available...
Waiting for deployment "podinfo" rollout to finish: 1 of 2 updated replicas are available...
deployment "podinfo" successfully rolled out
-------------\n
----

****You’ll now have a test namespace running two deployments in each cluster - frontend and podinfo. podinfo has been configured slightly differently in each cluster with a different name and color so that we can tell where requests are going.

To see what it looks like from the west cluster right now, you can run:

----
official suggest do this. but i just edit the service to LoadBalancer.
kubectl --context=west -n test port-forward svc/frontend 8080
curl http://localhost:8080
{
  "hostname": "podinfo-5c8cf55777-zbfls",
  "version": "4.0.2",
  "revision": "b4138fdb4dce7b34b6fc46069f70bb295aa8963c",
  "color": "#6c757d",
  "logo": "https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif",
  "message": "greetings from west",
  "goos": "linux",
  "goarch": "amd64",
  "runtime": "go1.14.3",
  "num_goroutine": "8",
  "num_cpu": "4"
}


kubectl --context=east -n test edit svc/frontend
kubectl --context=west -n test edit svc/frontend
----

=== export service
*** To make sure sensitive services are not mirrored and cluster performance is impacted by the creation or deletion of services, we require that services be explicitly exported. For the purposes of this guide, we will be exporting the podinfo service from the east cluster to the west cluster. To do this, we must first export the podinfo service in the east cluster. You can do this by adding the mirror.linkerd.io/exported label:

----
kubectl --context=east label svc -n test podinfo mirror.linkerd.io/exported=true
----

*** Check out the service that was just created by the service mirror controller!
----
[root@devopstesting linkerd]# kubectl --context=west -n test get svc podinfo-east
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
podinfo-east   ClusterIP   10.108.8.146   <none>        9898/TCP,9999/TCP   84s
[root@devopstesting linkerd]# kubectl --context=west -n test get svc
NAME           TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)             AGE
frontend       LoadBalancer   10.108.7.40    34.122.22.191   8080:31247/TCP      79m
podinfo        ClusterIP      10.108.4.149   <none>          9898/TCP,9999/TCP   79m
podinfo-east   ClusterIP      10.108.8.146   <none>          9898/TCP,9999/TCP   102s
[root@devopstesting linkerd]# kubectl --context=east -n test get svc
NAME       TYPE           CLUSTER-IP    EXTERNAL-IP    PORT(S)             AGE
frontend   LoadBalancer   10.48.5.116   35.232.45.49   8080:31092/TCP      79m
podinfo    ClusterIP      10.48.1.173   <none>         9898/TCP,9999/TCP   79m
----

*** From the architecture, you’ll remember that the service mirror component is doing more than just moving services over. It is also managing the endpoints on the mirrored service. To verify that is setup correctly, you can check the endpoints in west and verify that they match the gateway’s public IP address in east.

----
[root@devopstesting linkerd]# kubectl --context=west -n test get endpoints podinfo-east \
>   -o 'custom-columns=ENDPOINT_IP:.subsets[*].addresses[*].ip'
ENDPOINT_IP
104.154.46.246
[root@devopstesting linkerd]# kubectl --context=east -n linkerd-multicluster get svc linkerd-gateway \
>   -o "custom-columns=GATEWAY_IP:.status.loadBalancer.ingress[*].ip"
GATEWAY_IP
104.154.46.246
----

*** At this point, we can hit the podinfo service in east from the west cluster. This requires the client to be meshed, so let’s run curl from within the frontend pod:
"apk add curl" . I am use centos. needn't this. just use below command is OK.
----
[root@devopstesting linkerd]# kubectl --context=west -n test exec -c nginx -it   $(kubectl --context=west -n test get po -l app=frontend \
    --no-headers -o custom-columns=:.metadata.name)   -- /bin/sh -c "curl http://podinfo-east:9898"
{
  "hostname": "podinfo-57b4899f7d-lvvzz",
  "version": "4.0.2",
  "revision": "b4138fdb4dce7b34b6fc46069f70bb295aa8963c",
  "color": "#007bff",
  "logo": "https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif",
  "message": "greetings from east",
  "goos": "linux",
  "goarch": "amd64",
  "runtime": "go1.14.3",
  "num_goroutine": "9",
  "num_cpu": "2"
----

*** you can also reach this from your browser at http://localhost:8080/east. Refresh a couple times and you’ll be able to get metrics from linkerd stat as well.
note: for me, i install ingress-controller, export webui by ingress. 
here is my repo. just clone it . and edit deploy.yaml for updating export nodeport to loadbalancer since i am using gcp. we can see the external ip. then edit ingress.yaml, export the host as "dashboard.east.com". then apply it . then add ns by edit hosts file in my local.  we need two ui . one is for west context cluster. the other is for east. for west we need to do it again. but export hosts for ingress as "dashboard.west.com" and add ns to my local hosts file.

----
https://github.com/warrenlucky/linkerd-warren-test.git
----


*** now you can grafana. explorer http://dashboard.west.com/grafana

*** to check security
note: I got some error like below because permission. need to add permission first. add clusterrolebinding for two cluster. both west and east
error like.
HTTP error, status Code [403] (unexpected API response: {"error":"tap authorization failed (not authorized to access deployments.tap.linkerd.io)

----
[root@devopstesting linkerd-warren-test]# kubectl --context west create clusterrolebinding $(whoami)-cluster-admin --clusterrole=cluster-admin --user=$(gcloud config get-value account)
clusterrolebinding.rbac.authorization.k8s.io/root-cluster-admin created
[root@devopstesting linkerd-warren-test]# kubectl --context east create clusterrolebinding $(whoami)-cluster-admin --clusterrole=cluster-admin --user=$(gcloud config get-value account)

[root@devopstesting linkerd-warren-test]# linkerd --context=west -n test tap deploy/frontend | grep "$(kubectl --context=east -n linkerd-multicluster get svc linkerd-gateway \
    -o "custom-columns=GATEWAY_IP:.status.loadBalancer.ingress[*].ip")"
req id=0:0 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :method=GET :authority=podinfo.test.svc.cluster.local:9898 :path=/
rsp id=0:0 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :status=200 latency=3253µs
end id=0:0 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true duration=245µs response-length=382B
req id=0:2 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :method=GET :authority=podinfo.test.svc.cluster.local:9898 :path=/
rsp id=0:2 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :status=200 latency=3597µs
end id=0:2 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true duration=278µs response-length=382B
req id=0:4 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :method=GET :authority=podinfo.test.svc.cluster.local:9898 :path=/
rsp id=0:4 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :status=200 latency=5641µs
end id=0:4 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true duration=112µs response-length=382B
req id=0:6 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :method=GET :authority=podinfo.test.svc.cluster.local:9898 :path=/
rsp id=0:6 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :status=200 latency=3419µs
end id=0:6 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true duration=62µs response-length=382B
req id=0:8 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :method=GET :authority=podinfo.test.svc.cluster.local:9898 :path=/
rsp id=0:8 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true :status=200 latency=3392µs
end id=0:8 proxy=out src=10.104.0.9:58580 dst=104.154.46.246:4143 tls=true duration=49µs response-length=382B
----
